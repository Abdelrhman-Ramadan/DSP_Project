{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0fa830-4cec-4c44-b2d3-7b412bfd3b6a",
   "metadata": {},
   "source": [
    "# Distance-based Speech Recognition\n",
    "\n",
    "This project implements a **distance-based speech recognition** system using **Librosa**, **NumPy**, and **Matplotlib**. The system recognizes speech commands from the **Mini Speech Commands Dataset** by performing feature extraction and using **Euclidean** and **Cosine distance** to classify input audio signals.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Project Description](#project-description)\n",
    "- [Dependencies](#dependencies)\n",
    "- [Methodology](#methodology)\n",
    "  - [1. Divide the Signal into Frames](#1-divide-the-signal-into-frames)\n",
    "  - [2. Compute the Spectrum Using FFT](#2-compute-the-spectrum-using-fft)\n",
    "  - [3. Compute Class Prototypes](#3-compute-class-prototypes)\n",
    "  - [4. Testing](#4-testing)\n",
    "  - [5. Predict Class](#5-predict-class)\n",
    "  - [6. Compute Accuracy](#6-compute-accuracy)\n",
    "  - [7. Euclidean Distance](#7-euclidean-distance)\n",
    "  - [8. Cosine Distance](#8-cosine-distance)\n",
    "- [Conclusion](#conclusion)\n",
    "\n",
    "## Project Description\n",
    "\n",
    "This project aims to build a simple speech recognition system using the **Mini Speech Commands Dataset**. The model is based on the **Euclidean distance** and **Cosine distance** between the test features and class prototypes computed during training. The system extracts **spectral features** from the audio signal, computes **prototypes** for each class, and classifies test signals by calculating the distance to each class prototype.\n",
    "\n",
    "### Key Features:\n",
    "- **FFT (Fast Fourier Transform)** is used for frequency domain analysis.\n",
    "- **Euclidean distance** and **Cosine distance** are used for classification.\n",
    "- The model is trained and evaluated on a set of predefined speech commands.\n",
    "\n",
    "---\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "This project requires the following Python libraries:\n",
    "- **Librosa**: For audio processing and feature extraction.\n",
    "- **NumPy**: For numerical operations.\n",
    "- **Matplotlib**: For plotting and visualizations.\n",
    "- **SciPy**: For metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### 1. Divide the Signal into Frames\n",
    "\n",
    "We begin by dividing the audio signal into overlapping frames. This allows us to analyze smaller segments of the signal to capture both **temporal** and **spectral** information.\n",
    "\n",
    "- **Frame length**: 400 samples (~25ms at 16kHz sampling rate)\n",
    "- **Hop length**: (~10ms)\n",
    "\n",
    "These frame lengths are chosen to ensure that the window of analysis captures short-term spectral changes, as speech signals tend to change rapidly in short durations.\n",
    "\n",
    "#### Equation:\n",
    "If the audio signal $x(t)$ is of length $N$, then the total number of frames $F$ will be:\n",
    "\n",
    "$$\n",
    "F = \\frac{N - \\text{frame\\_length}}{\\text{hop\\_length}} + 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Compute the Spectrum Using FFT\n",
    "\n",
    "Next, we compute the **Fast Fourier Transform (FFT)** for each frame. The FFT transforms the time-domain signal into the frequency domain, allowing us to analyze the spectral content.\n",
    "\n",
    "- **FFT size**: 512 points\n",
    "\n",
    "The choice of $n_{\\text{fft}} = 512$ allows us to capture enough frequency information (e.g., for speech analysis) while maintaining a balance between **frequency resolution** and **computational efficiency**.\n",
    "\n",
    "#### Formula:\n",
    "The FFT of a signal $x(t)$ is defined as:\n",
    "\n",
    "$$\n",
    "X(f) = \\sum_{n=0}^{N-1} x(n) \\cdot e^{-j 2\\pi f n / N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x(n)$ is the signal in the time domain\n",
    "- $X(f)$ is the signal in the frequency domain\n",
    "- $N$ is the length of the signal or FFT size\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Compute Class Prototypes\n",
    "\n",
    "During training, we compute a **prototype** for each speech class. This is done by averaging the FFT frames of all audio samples belonging to a particular class.\n",
    "\n",
    "- **Training set**: We compute the **mean** of the FFT features for each class.\n",
    "- **Prototype for class $c$**: The mean of the FFT frames from all samples of class $c$.\n",
    "\n",
    "#### Equation:\n",
    "If the FFT frames for class $c$ are $X_{c1}, X_{c2}, \\dots, X_{cN_c}$, then the prototype for class $c$ is:\n",
    "\n",
    "$$\n",
    "\\mu_c = \\frac{1}{N_c} \\sum_{i=1}^{N_c} X_{ci}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N_c$ is the number of samples in class $c$\n",
    "- $X_{ci}$ are the FFT frames of the $i$-th sample in class $c$\n",
    "- $\\mu_c$ is the prototype (average frame) for class $c$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Testing\n",
    "\n",
    "For testing, we compute the FFT features of the test signal and calculate the **Euclidean distance** and **Cosine distance** between the test features and each class prototype.\n",
    "\n",
    "#### Euclidean Distance Formula:\n",
    "The Euclidean distance between two vectors $x$ and $y$ is given by:\n",
    "\n",
    "$$\n",
    "D(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ and $y_i$ are the elements of the vectors $x$ and $y$, respectively\n",
    "- $n$ is the length of the vectors\n",
    "\n",
    "We compute this distance for each class and select the class with the smallest distance.\n",
    "\n",
    "#### Cosine Distance Formula:\n",
    "The Cosine distance between two vectors $x$ and $y$ is given by:\n",
    "\n",
    "$$\n",
    "D_{\\text{cos}}(x, y) = 1 - \\frac{x \\cdot y}{\\|x\\| \\|y\\|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x \\cdot y$ is the dot product of $x$ and $y$\n",
    "- $\\|x\\|$ and $\\|y\\|$ are the magnitudes (norms) of the vectors $x$ and $y$, respectively\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Predict Class\n",
    "\n",
    "After computing the distances for all classes, we select the class with the smallest distance to the test sample. This is the predicted class.\n",
    "\n",
    "```python\n",
    "def predict_class(distances):\n",
    "    return min(distances, key=distances.get)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Compute Accuracy\n",
    "\n",
    "Finally, the modelâ€™s performance is evaluated using **accuracy**, which is the proportion of correctly predicted classes compared to the total number of test samples.\n",
    "\n",
    "#### Accuracy Formula:\n",
    "The accuracy is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- A correct prediction means the predicted class matches the true class label.\n",
    "\n",
    "```python\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    correct = sum(p == t for p, t in zip(predictions, true_labels))\n",
    "    return correct / len(true_labels)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this project, we built a **distance-based speech recognition** system that leverages **FFT** for feature extraction and both **Euclidean distance** and **Cosine distance** for classification. While this approach is simple, it provides a strong foundation for more advanced speech recognition techniques.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16701e9-f451-4799-af48-1b09955b2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean, cosine, cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe279cb-4a92-46ef-b9dc-8899e161aabe",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6f6db1-74ca-4995-b722-7c8c3b7ae4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_split(data_dir, test_size=0.2, seed=20):\n",
    "    # Initialize lists for data and labels\n",
    "    train_signals, train_labels = [], []\n",
    "    test_signals, test_labels = [], []\n",
    "\n",
    "    # Get class folders\n",
    "    classes = sorted(os.listdir(data_dir))  # Classes are folder names\n",
    "    class_to_label = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_folder = os.path.join(data_dir, cls)\n",
    "        wav_files = [os.path.join(class_folder, f) for f in os.listdir(class_folder) if f.endswith('.wav')]\n",
    "\n",
    "        # Shuffle and split data into train and test\n",
    "        random.seed(seed)\n",
    "        random.shuffle(wav_files)\n",
    "        split_idx = int(len(wav_files) * (1 - test_size))\n",
    "\n",
    "        train_files = wav_files[:split_idx]\n",
    "        test_files = wav_files[split_idx:]\n",
    "\n",
    "        # Load WAV files and labels\n",
    "        for wav_file in train_files:\n",
    "            signal, _ = librosa.load(wav_file, sr=None)  # Load audio file\n",
    "            train_signals.append(signal)\n",
    "            train_labels.append(class_to_label[cls])\n",
    "\n",
    "        for wav_file in test_files:\n",
    "            signal, _ = librosa.load(wav_file, sr=None)\n",
    "            test_signals.append(signal)\n",
    "            test_labels.append(class_to_label[cls])\n",
    "\n",
    "    return (np.array(train_signals, dtype=object), np.array(train_labels)), \\\n",
    "           (np.array(test_signals, dtype=object), np.array(test_labels))\n",
    "\n",
    "\n",
    "def frame_signal(signal, frame_size, hop_size):\n",
    "    \"\"\"\n",
    "    Divide a signal into overlapping frames.\n",
    "\n",
    "    Args:\n",
    "        signal: The input audio signal (1D NumPy array).\n",
    "        frame_size: Length of each frame in samples.\n",
    "        hop_size: Step size between frames in samples.\n",
    "\n",
    "    Returns:\n",
    "        A 2D NumPy array where each row is a frame.\n",
    "    \"\"\"\n",
    "    frames = librosa.util.frame(signal, frame_length=frame_size, hop_length=hop_size).T \n",
    "    return frames # shape (98, 400)\n",
    "\n",
    "def compute_fft(frames):\n",
    "    \"\"\"\n",
    "    Compute the FFT spectrum for each frame.\n",
    "\n",
    "    Args:\n",
    "        frames: A 2D NumPy array where each row is a frame.\n",
    "\n",
    "    Returns:\n",
    "        A 2D NumPy array of spectra (magnitude values) for each frame.\n",
    "    \"\"\"\n",
    "    return np.abs(np.fft.rfft(frames, axis=1))\n",
    "\n",
    "def compute_class_prototypes(train_data, labels):\n",
    "    \"\"\"\n",
    "    Compute the average (prototype) for each speech class based on the FFT frames.\n",
    "    \n",
    "    Args:\n",
    "        train_data: A list or 1D NumPy array where each element is a 2D array (e.g., FFT features of shape `(98, 201)`).\n",
    "        labels: A 1D NumPy array of labels corresponding to each element in train_data.\n",
    "\n",
    "    Returns:\n",
    "        class_prototypes: A dictionary where keys are labels and values are the mean of all frames for that class.\n",
    "    \"\"\"\n",
    "    class_prototypes = {}\n",
    "\n",
    "    # Get unique labels\n",
    "    for label in np.unique(labels):\n",
    "        # Find indices of frames corresponding to the current label\n",
    "        indices = np.where(labels == label)[0]\n",
    "        \n",
    "        # Collect all frames for the current label\n",
    "        class_frames = [train_data[i] for i in indices]\n",
    "\n",
    "        class_frames_mean = np.array([np.mean(frame, axis=0) for frame in class_frames])\n",
    "        \n",
    "        # Compute the mean across all frames (averaging across the first axis)\n",
    "        class_prototypes[label] = np.mean(class_frames_mean, axis=0)\n",
    "    \n",
    "    return class_prototypes\n",
    "\n",
    "def cosine_distance(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return 1 - (dot_product / (norm_vec1 * norm_vec2))\n",
    "\n",
    "def compute_distances(test_features, class_prototypes, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Compute distances between test features and class prototypes.\n",
    "\n",
    "    Args:\n",
    "        test_features: A 2D array of shape `(num_frames, num_features)` for a single test sample.\n",
    "        class_prototypes: A dictionary where keys are labels and values are 2D arrays (prototypes).\n",
    "        metric: The distance metric to use ('euclidean', 'cosine', 'hamming', etc.).\n",
    "\n",
    "    Returns:\n",
    "        distances: A dictionary where keys are class labels and values are distances.\n",
    "    \"\"\"\n",
    "    distances = {}\n",
    "    for label, prototype in class_prototypes.items():\n",
    "        # Compute distances between test_features and the prototype (frame-wise)\n",
    "        test_features_mean = np.mean(test_features, axis=0)\n",
    "        \n",
    "        # frame_distances = cdist(test_features_mean, prototype, metric=metric)\n",
    "        frame_distances = cdist(test_features_mean.reshape(1, -1), prototype.reshape(1, -1), metric=metric)\n",
    "        \n",
    "        # Average the frame-wise distances for the final distance\n",
    "        distances[label] = frame_distances.mean()\n",
    "    \n",
    "    return distances\n",
    "\n",
    "\n",
    "def predict_class(distances):\n",
    "    return min(distances, key=distances.get)\n",
    "\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    correct = sum(p == t for p, t in zip(predictions, true_labels))\n",
    "    return correct / len(true_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307980cb-0e13-458c-a7e2-1a90d7f742db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "!ls -1A mini_speech_commands/mini_speech_commands/down | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9f5f07-ae95-42ef-bd15-3c0d443d5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5600, Testing samples: 2400\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path\n",
    "data_dir = \"mini_speech_commands/mini_speech_commands\"\n",
    "\n",
    "# Load data and split\n",
    "(train_signal, train_labels), (test_signal, test_labels) = load_data_and_split(data_dir, test_size=0.2)\n",
    "\n",
    "print(f\"Training samples: {len(train_signal)}, Testing samples: {len(test_signal)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b061380-ec07-422c-84e6-24a5a0888dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,\n",
       " array([ 0.00036621,  0.00057983, -0.00036621, ..., -0.00030518,\n",
       "        -0.00021362, -0.0005188 ], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test_sample\n",
    "tst_signal, sr = librosa.load('mini_speech_commands/mini_speech_commands/go/0132a06d_nohash_2.wav', sr=None)\n",
    "len(tst_signal), tst_signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8091c8b4-350b-4562-aafc-bb4619fbfba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide the signals to frames\n",
    "sr = 16000\n",
    "frame_size = int(0.025 * sr)  # 25ms frame (convert to samples)\n",
    "hop_size = int(0.010 * sr) # 10ms hop (convert to samples)\n",
    "\n",
    "train_signal_frames_fft = []\n",
    "for signal in train_signal:\n",
    "    frames = frame_signal(signal, frame_size, hop_size)\n",
    "    fft_frames = compute_fft(frames) \n",
    "    train_signal_frames_fft.append(fft_frames) # Max, Min = (98, 201), (41, 201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f96ffdd-9bd6-4e20-8377-ec22a46cfe2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 640), (97, 321), (97, 321), 5600)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape, fft_frames.shape, train_signal_frames_fft[0].shape, len(train_signal_frames_fft)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2e4940d-ab2d-405e-892c-4e245c0e4413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 321), (39, 321))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_shape = tuple(np.max([frame.shape for frame in train_signal_frames_fft], axis=0))\n",
    "min_shape = tuple(np.min([frame.shape for frame in train_signal_frames_fft], axis=0))\n",
    "max_shape, min_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e226d0c5-fe20-4342-a09d-b105961c5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average of the frames for each speech class\n",
    "class_prototypes = compute_class_prototypes(train_signal_frames_fft, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "316f541d-e948-44b9-b585-cd3c5b517095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prototypes[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684c06b-3465-4b3c-be33-2ffa29585df9",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95c4e9a4-08b1-4ded-9cb7-c5c368cc6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signal_frames_fft = []\n",
    "for signal in test_signal:\n",
    "    frames = frame_signal(signal, frame_size, hop_size)\n",
    "    fft_frames = compute_fft(frames)\n",
    "    test_signal_frames_fft.append(fft_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7214cbc4-ad78-4709-a3b8-0f109a4d7a5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Predictions with cosine distance\n",
    "# predictions = []\n",
    "# for test_frame in test_signal_frames_fft:\n",
    "#     distances = {}\n",
    "#     for label, prototype in class_prototypes.items():\n",
    "#         # Compute distances between test_features and the prototype (frame-wise)\n",
    "#         test_frame_mean = np.mean(test_frame, axis=0)  # Average the frame-wise features for the test frame\n",
    "        \n",
    "#         # Compute cosine distance between the test frame and the prototype\n",
    "#         frame_distances = cosine_distance(test_frame_mean, prototype)\n",
    "        \n",
    "#         # Since frame_distances is a float, just assign it directly\n",
    "#         distances[label] = frame_distances\n",
    "#     predictions.append(predict_class(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1e7c042-40a2-4b0b-9cbc-63118a320088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Predict for test set\n",
    "predictions = []\n",
    "for test_frame in test_signal_frames_fft:\n",
    "    distances = compute_distances(test_frame, class_prototypes, metric='cosine') # euclidean, hamming, cosine\n",
    "    predictions.append(predict_class(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5842d43b-0191-4ddb-b889-0611714340b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84, 321), (321,), (97, 321))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_signal_frames_fft[0].shape, class_prototypes[7].shape, max_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c35acd4-951a-41e9-9e55-e721bc8d4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Compute accuracy\n",
    "accuracy = compute_accuracy(predictions, test_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2524b76-82fa-492e-afa1-5d893feee1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
